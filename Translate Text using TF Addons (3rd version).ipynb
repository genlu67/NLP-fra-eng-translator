{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ac361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\App\\Miniconda3\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os \n",
    "\n",
    "import time\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4f10dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'fra-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip',\n",
    "    extract=True, cache_dir = \"D:/Programming/Python/NLP tutorial\")\n",
    "\n",
    "path_to_file = pathlib.Path(path_to_zip).parent/'fra-eng/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63474969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    text = path.read_text(encoding='utf-8')\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "\n",
    "    inp = [inp for targ, inp in pairs]\n",
    "    targ = [targ for targ, inp in pairs]\n",
    "\n",
    "    return targ, inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d64e46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il est peut-être impossible d'obtenir un Corpus complètement dénué de fautes, étant donnée la nature de ce type d'entreprise collaborative. Cependant, si nous encourageons les membres à produire des phrases dans leurs propres langues plutôt que d'expérimenter dans les langues qu'ils apprennent, nous pourrions être en mesure de réduire les erreurs.\n"
     ]
    }
   ],
   "source": [
    "targ, inp = load_data(path_to_file)\n",
    "print(inp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bff4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_lower_and_split_punct:\n",
    "    def __init__(self, start = True, end = True):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        \n",
    "    def __call__(self, text):\n",
    "        # Split accented characters.\n",
    "        text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "        text = tf.strings.lower(text)\n",
    "        # Keep space, a to z, and select punctuation.\n",
    "        text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "        # Add spaces around punctuation.\n",
    "        text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "        # Strip whitespace.\n",
    "        text = tf.strings.strip(text)\n",
    "        if self.start == True:\n",
    "            text = tf.strings.join(['[START]', text], separator=' ')\n",
    "        if self.end == True:\n",
    "            text = tf.strings.join([text, '[END]'], separator=' ')\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc1f052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab_size = 5000\n",
    "BUFFER_SIZE = 32000\n",
    "BATCH_SIZE = 64\n",
    "max_input_sequence = 20\n",
    "max_output_sequence = 20\n",
    "\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct(),\n",
    "    max_tokens=max_vocab_size,\n",
    "    output_sequence_length=max_input_sequence)\n",
    "\n",
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct(),\n",
    "    max_tokens=max_vocab_size,\n",
    "    output_sequence_length=max_output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8444d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text_processor.adapt(inp)\n",
    "output_text_processor.adapt(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d63d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_data,valid_input_data, train_output_data, valid_output_data = train_test_split(inp, targ, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f9b9a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_output_data))\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((valid_input_data, valid_output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc779f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "    return (ds\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y : (input_text_processor(x), output_text_processor(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "train_batches = make_batches(train_dataset)\n",
    "valid_batches = make_batches(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5214ec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
       " array([[   2,   26,   57, ...,    0,    0,    0],\n",
       "        [   2,   15,    7, ...,    0,    0,    0],\n",
       "        [   2,    5,  168, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2,   15,    7, ...,    0,    0,    0],\n",
       "        [   2,   15,    7, ...,    0,    0,    0],\n",
       "        [   2,   30, 3997, ...,    0,    0,    0]], dtype=int64)>,\n",
       " <tf.Tensor: shape=(64, 20), dtype=int64, numpy=\n",
       " array([[   2,    5,  120, ...,    0,    0,    0],\n",
       "        [   2,  142, 1082, ...,    0,    0,    0],\n",
       "        [   2,    5,  120, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [   2,   14,  310, ...,    0,    0,    0],\n",
       "        [   2,   14,  244, ...,    0,    0,    0],\n",
       "        [   2,   52,  231, ...,    0,    0,    0]], dtype=int64)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_output_batch = next(iter(train_batches))\n",
    "example_input_batch, example_output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7eb6838",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "units = 1024\n",
    "num_examples = 30000\n",
    "steps_per_epoch = num_examples//BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe9dff86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                              return_sequences = True,\n",
    "                                              return_state = True,\n",
    "                                              recurrent_initializer = \"glorot_uniform\")\n",
    "        \n",
    "    def call(self, inp, hidden = None):\n",
    "        inp = self.embedding(inp)\n",
    "        output, h, c = self.lstm_layer(inp, initial_state = hidden)\n",
    "        return output, h, c\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.fill((self.batch_size, self.enc_units), 0.), tf.fill((self.batch_size, self.enc_units), 0.)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6fde3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\n",
      "Encoder h vecotr shape: (batch size, units) (64, 1024)\n",
      "Encoder c vector shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(max_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
    "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfc4e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, attention_type = \"luong\"):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.attention_type = attention_type\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
    "        \n",
    "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
    "                                                              None, self.batch_size*[max_input_sequence], self.attention_type)\n",
    "        \n",
    "        self.rnn_cell = self.build_rnn_cell(batch_size)\n",
    "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, \n",
    "                                                sampler=self.sampler, \n",
    "                                                output_layer=self.fc)\n",
    "    def build_rnn_cell(self, batch_size):\n",
    "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
    "                                                self.attention_mechanism, \n",
    "                                                attention_layer_size=self.dec_units)\n",
    "        return rnn_cell\n",
    "    \n",
    "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
    "        if(attention_type=='bahdanau'):\n",
    "            return tfa.seq2seq.BahdanauAttention(units=dec_units, \n",
    "                                                 memory=memory, \n",
    "                                                 memory_sequence_length=memory_sequence_length)\n",
    "        else:\n",
    "            return tfa.seq2seq.LuongAttention(units=dec_units, \n",
    "                                              memory=memory, \n",
    "                                              memory_sequence_length=memory_sequence_length)\n",
    "        \n",
    "    def build_initial_state(self, batch_size, encoder_state, dtype):\n",
    "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_size, dtype=dtype)\n",
    "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
    "        return decoder_initial_state\n",
    "    \n",
    "    def call(self, inputs, initial_state):\n",
    "        x = self.embedding(inputs)\n",
    "        outputs, _, _ = self.decoder(x, initial_state = initial_state,\n",
    "                                    sequence_length = self.batch_size*[max_input_sequence-1])\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0cb4aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Outputs Shape:  (64, 19, 5000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(max_vocab_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
    "sample_x = tf.random.uniform((BATCH_SIZE, max_output_sequence))\n",
    "decoder.attention_mechanism.setup_memory(sample_output)\n",
    "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
    "\n",
    "sample_decoder_outputs = decoder(sample_x, initial_state)\n",
    "\n",
    "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "465078db",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam()\n",
    "\n",
    "def loss_func(real, pred):\n",
    "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
    "    mask = tf.logical_not(tf.math.equal(real,0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = mask* loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e88687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0db7873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_input = targ[ : , :-1 ]\n",
    "        real = targ[ : , 1: ]\n",
    "        \n",
    "        decoder.attention_mechanism.setup_memory(enc_output)\n",
    "        \n",
    "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
    "        pred = decoder(dec_input, decoder_initial_state)\n",
    "        logits = pred.rnn_output\n",
    "        loss = loss_func(real, logits)\n",
    "        \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4613c7af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.6282\n",
      "Epoch 1 Batch 100 Loss 1.9178\n",
      "Epoch 1 Batch 200 Loss 1.9207\n",
      "Epoch 1 Batch 300 Loss 1.8502\n",
      "Epoch 1 Batch 400 Loss 1.6136\n",
      "Epoch 1 Loss 1.8780\n",
      "Time taken for 1 epoch 115.44803738594055 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.4823\n",
      "Epoch 2 Batch 100 Loss 1.6008\n",
      "Epoch 2 Batch 200 Loss 1.4328\n",
      "Epoch 2 Batch 300 Loss 1.2871\n",
      "Epoch 2 Batch 400 Loss 1.3157\n",
      "Epoch 2 Loss 1.4232\n",
      "Time taken for 1 epoch 105.48214387893677 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 1.3612\n",
      "Epoch 3 Batch 100 Loss 1.2288\n",
      "Epoch 3 Batch 200 Loss 1.2377\n",
      "Epoch 3 Batch 300 Loss 0.9992\n",
      "Epoch 3 Batch 400 Loss 0.9637\n",
      "Epoch 3 Loss 1.1389\n",
      "Time taken for 1 epoch 104.0552990436554 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.8672\n",
      "Epoch 4 Batch 100 Loss 0.7556\n",
      "Epoch 4 Batch 200 Loss 0.7260\n",
      "Epoch 4 Batch 300 Loss 0.8262\n",
      "Epoch 4 Batch 400 Loss 0.8023\n",
      "Epoch 4 Loss 0.7979\n",
      "Time taken for 1 epoch 106.8150954246521 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.5736\n",
      "Epoch 5 Batch 100 Loss 0.6781\n",
      "Epoch 5 Batch 200 Loss 0.5878\n",
      "Epoch 5 Batch 300 Loss 0.5498\n",
      "Epoch 5 Batch 400 Loss 0.4776\n",
      "Epoch 5 Loss 0.6296\n",
      "Time taken for 1 epoch 105.73368859291077 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.5675\n",
      "Epoch 6 Batch 100 Loss 0.5023\n",
      "Epoch 6 Batch 200 Loss 0.5006\n",
      "Epoch 6 Batch 300 Loss 0.5830\n",
      "Epoch 6 Batch 400 Loss 0.5390\n",
      "Epoch 6 Loss 0.5344\n",
      "Time taken for 1 epoch 106.12216019630432 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.5368\n",
      "Epoch 7 Batch 100 Loss 0.4879\n",
      "Epoch 7 Batch 200 Loss 0.4383\n",
      "Epoch 7 Batch 300 Loss 0.4030\n",
      "Epoch 7 Batch 400 Loss 0.6045\n",
      "Epoch 7 Loss 0.4735\n",
      "Time taken for 1 epoch 107.94366073608398 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.3627\n",
      "Epoch 8 Batch 100 Loss 0.4488\n",
      "Epoch 8 Batch 200 Loss 0.4009\n",
      "Epoch 8 Batch 300 Loss 0.4605\n",
      "Epoch 8 Batch 400 Loss 0.5212\n",
      "Epoch 8 Loss 0.4244\n",
      "Time taken for 1 epoch 110.65079975128174 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.3869\n",
      "Epoch 9 Batch 100 Loss 0.3515\n",
      "Epoch 9 Batch 200 Loss 0.3416\n",
      "Epoch 9 Batch 300 Loss 0.4852\n",
      "Epoch 9 Batch 400 Loss 0.3350\n",
      "Epoch 9 Loss 0.3843\n",
      "Time taken for 1 epoch 102.58403277397156 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.2739\n",
      "Epoch 10 Batch 100 Loss 0.3926\n",
      "Epoch 10 Batch 200 Loss 0.3587\n",
      "Epoch 10 Batch 300 Loss 0.3117\n",
      "Epoch 10 Batch 400 Loss 0.3894\n",
      "Epoch 10 Loss 0.3522\n",
      "Time taken for 1 epoch 103.09815549850464 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(train_batches.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d2d892bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "    def __init__(self, encoder, decoder, input_text_processor, output_text_processor):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        \n",
    "        self.output_token_string_from_index = (\n",
    "            tf.keras.layers.StringLookup(\n",
    "                vocabulary=output_text_processor.get_vocabulary(),\n",
    "                mask_token='',\n",
    "                invert=True))\n",
    "        \n",
    "        index_from_string = tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "        \n",
    "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "        \n",
    "        token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "        token_mask[np.array(token_mask_ids)] = True\n",
    "        self.token_mask = token_mask\n",
    "        \n",
    "        self.start_token = index_from_string(tf.constant('[START]'))\n",
    "        self.end_token = index_from_string(tf.constant('[END]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8742acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16716\\1989862978.py:19: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(\n",
    "    encoder= encoder,\n",
    "    decoder= decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2e96106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_text(self, result_tokens):\n",
    "    print(\"tokens : \",result_tokens)\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                       axis=1, separator=' ')\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    return result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fcac19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tokens_to_text = tokens_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d283db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_sample(self, sentence):\n",
    "    inputs = input_text_processor(sentence)\n",
    "    \n",
    "    inference_batch_size = tf.shape(inputs)[0]\n",
    "    \n",
    "    enc_start_state = [tf.fill([inference_batch_size, units], 0.), tf.fill([inference_batch_size,units], 0.)]\n",
    "    \n",
    "    print(inputs)\n",
    "    enc_out, enc_h, enc_c = self.encoder(inputs, enc_start_state)\n",
    "    \n",
    "    dec_h = enc_h\n",
    "    dec_c = enc_c\n",
    "    \n",
    "    start_tokens = tf.fill([inference_batch_size],  2)\n",
    "    end_token = 3 #output_text_processor.get_vocabulary().index('[END]')\n",
    "    \n",
    "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "    \n",
    "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=self.decoder.rnn_cell, \n",
    "                                                sampler=greedy_sampler, \n",
    "                                                output_layer=self.decoder.fc)\n",
    "    self.decoder.attention_mechanism.setup_memory(enc_out)\n",
    "    \n",
    "    decoder_initial_state = self.decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
    "    \n",
    "    decoder_embedding_matrix = self.decoder.embedding.variables[0]\n",
    "    \n",
    "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, \n",
    "                                     start_tokens = start_tokens, \n",
    "                                     end_token= end_token, \n",
    "                                     initial_state=decoder_initial_state)\n",
    "    return outputs.sample_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87e35a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.tf_sample = tf_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f093d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "def __call__(self, sentence):\n",
    "    sample_id = self.tf_sample(sentence)\n",
    "    return self.tokens_to_text(sample_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c7eaa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Translator.__call__ = __call__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83d390a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = tf.constant([\n",
    "    \"Je veux un taxi près de l'aéroport\", \n",
    "    \"ou est la table\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66ae6f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"text_vectorization/RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, 20), dtype=int64)\n",
      "tokens :  Tensor(\"basic_decoder/decoder/transpose_1:0\", shape=(None, None), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'i want a cab near the airport . [END]',\n",
       "       b'where is the table ? [END] . [END] .'], dtype=object)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = translator(input_text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1882c49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportTranslator(tf.Module):\n",
    "  def __init__(self, translator):\n",
    "    self.translator = translator\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
    "  def __call__(self, sentence):\n",
    "    result = self.translator(sentence)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45a419ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = ExportTranslator(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88a28966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"text_vectorization/RaggedToTensor/RaggedTensorToTensor:0\", shape=(None, 20), dtype=int64)\n",
      "tokens :  Tensor(\"basic_decoder/decoder/transpose_1:0\", shape=(None, None), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, decoder_layer_call_fn, decoder_layer_call_and_return_conditional_losses, embedding_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: attention_translator\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: attention_translator\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(translator, export_dir = \"attention_translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bba79dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded = tf.saved_model.load(\"attention_translator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80540d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=\n",
       "array([b'i want a cab near the airport . [END]',\n",
       "       b'where is the table ? [END] . [END] .'], dtype=object)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reloaded(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d33e15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
